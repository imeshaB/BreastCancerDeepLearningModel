{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Uninstall any existing TensorFlow\n",
        "!pip uninstall tensorflow -y\n",
        "\n",
        "# Install TensorFlow 2.17\n",
        "!pip install tensorflow==2.17.0\n",
        "\n",
        "# Install Keras, Keras Tuner, and other required libraries\n",
        "!pip install keras keras-tuner scikit-learn matplotlib scikit-image imblearn\n",
        "\n",
        "# Verify the installation\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras Tuner version:\", kt.__version__)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnkMxDYjU5iH",
        "outputId": "6faaeece-5e9b-4bb7-b3c5-b46f9e9a0951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Successfully uninstalled tensorflow-2.17.0\n",
            "Collecting tensorflow==2.17.0\n",
            "  Using cached tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.2)\n",
            "Using cached tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-2.17.0\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Collecting keras-tuner\n",
            "  Using cached keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Collecting imblearn\n",
            "  Using cached imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.7.21)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.6.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Using cached keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Installing collected packages: keras-tuner, imblearn\n",
            "Successfully installed imblearn-0.0 keras-tuner-1.4.7\n",
            "TensorFlow version: 2.17.0\n",
            "Keras Tuner version: 1.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization ,Activation, GlobalAveragePooling2D,LayerNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, AdamW, Adamax\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2,l1_l2\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,ConfusionMatrixDisplay, roc_auc_score, precision_recall_curve\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "svazSuoNU6Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp3kDCtNVA3R",
        "outputId": "65e8028a-9797-4291-a75b-ff5830369629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load images and labels\n",
        "def load_images_and_labels(folder_path, image_size=(224, 224)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        try:\n",
        "            image = imread(file_path)\n",
        "            if image is not None:\n",
        "                image = resize(image, image_size, anti_aliasing=True)\n",
        "                # Normalize the image\n",
        "                image =normalize_image(image)\n",
        "\n",
        "                images.append(image)\n",
        "                label = file.rsplit('_', 1)[0]\n",
        "                labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "    return np.array(images), np.array(labels)\n"
      ],
      "metadata": {
        "id": "zg9-pDf6VDNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_image(image):\n",
        "    # Normalize the image to scale pixel values to [0, 1]\n",
        "    return image / 255.0\n"
      ],
      "metadata": {
        "id": "XxqaBMo2VIWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mean_std(images):\n",
        "    mean, variance = tf.nn.moments(images, axes=[0, 1, 2])\n",
        "    stddev = tf.sqrt(variance) + 1e-7  # Add small constant to avoid division by zero\n",
        "    return mean, stddev\n"
      ],
      "metadata": {
        "id": "n8QsZWSxpNsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_images(images, mean, stddev):\n",
        "    return (images - mean) / stddev\n"
      ],
      "metadata": {
        "id": "sPYgHuhCpRoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "folder_path ='/content/drive/MyDrive/Colab Notebooks/Model/Data/Mendeley/classificacao_binaria/images_new'\n",
        "\n",
        "images, labels = load_images_and_labels(folder_path)\n",
        "\n",
        "# Check the shape of images and labels\n",
        "print(\"Shape of images:\", images.shape)\n",
        "print(\"Shape of labels:\", labels.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5gHwwszVKkC",
        "outputId": "d8e12aca-428c-4358-bb79-64ce1bdc28a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of images: (7919, 224, 224, 3)\n",
            "Shape of labels: (7919,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean and stddev from the dataset\n",
        "mean, stddev = compute_mean_std(images)\n",
        "\n",
        "# Convert tensors to NumPy arrays\n",
        "mean_np = mean.numpy()\n",
        "stddev_np = stddev.numpy()\n",
        "print(\"Mean:\", mean_np)\n",
        "print(\"Stddev:\", stddev_np)\n",
        "\n",
        "# Save to files\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/Model/mean.npy', mean_np)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/Model/stddev.npy', stddev_np)\n"
      ],
      "metadata": {
        "id": "rwivo68ivuSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_np=np.load('/content/drive/MyDrive/Colab Notebooks/Model/mean.npy')\n",
        "stddev_np=np.load('/content/drive/MyDrive/Colab Notebooks/Model/stddev.npy')\n",
        "# Standardize images using computed mean and stddev\n",
        "standardized_images_np = standardize_images(images, mean_np, stddev_np)\n"
      ],
      "metadata": {
        "id": "U-IYrbrtvhR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "lbl_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "foErrrptWur8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights based on the original labels\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(lbl_encoded), y=lbl_encoded)\n",
        "class_weights_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "aHwLhxThrlIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best hyperparameters\n",
        "best_hyperparameters = {\n",
        "    'conv1_filters': 32,\n",
        "    'conv1_dropout': 0.1,\n",
        "    'conv2_dropout_0': 0.1,\n",
        "    'conv2_dropout_1': 0.2,\n",
        "    'conv3_dropout_0': 0.2,\n",
        "    'conv3_dropout_1': 0.1,\n",
        "    'fc_units_0': 384,\n",
        "    'fc_dropout_0': 0.3,\n",
        "    'fc_units_1': 448,\n",
        "    'fc_dropout_1': 0.5,\n",
        "    'fc_units_2': 512,\n",
        "    'fc_dropout_2': 0.6,\n",
        "    'learning_rate': 6.668164718461625e-05,\n",
        "    'weight_decay': 1.5185965795089264e-05\n",
        "}\n",
        "\n",
        "def build_final_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Block 1\n",
        "    model.add(Conv2D(\n",
        "        filters=best_hyperparameters['conv1_filters'],\n",
        "        kernel_size=(3, 3),\n",
        "        activation='relu',\n",
        "        padding='same',\n",
        "        input_shape=(224, 224, 3),\n",
        "        kernel_regularizer=l1_l2(\n",
        "            l1=1e-7,\n",
        "            l2=1e-4)\n",
        "    ))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(rate=best_hyperparameters['conv1_dropout']))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 2\n",
        "    for i in range(2):  # Two Conv2D layers\n",
        "        model.add(Conv2D(128,\n",
        "            kernel_size=(3, 3),\n",
        "            activation='relu',\n",
        "            padding='same',\n",
        "            kernel_regularizer=l1_l2(\n",
        "                l1=1e-7,\n",
        "                l2=1e-4)\n",
        "        ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(rate=best_hyperparameters[f'conv2_dropout_{i}']))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 3\n",
        "    for i in range(2):  # Two Conv2D layers\n",
        "        model.add(Conv2D(256,\n",
        "            kernel_size=(3, 3),\n",
        "            activation='relu',\n",
        "            padding='same',\n",
        "            kernel_regularizer=l1_l2(\n",
        "                l1=1e-7,\n",
        "                l2=1e-4)\n",
        "        ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(rate=best_hyperparameters[f'conv3_dropout_{i}']))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    for i in range(3):  # Three Dense layers\n",
        "        model.add(Dense(\n",
        "            units=best_hyperparameters[f'fc_units_{i}'],\n",
        "            activation='relu',\n",
        "            kernel_regularizer=l1_l2(\n",
        "                l1=1e-7,\n",
        "                l2=1e-4)\n",
        "        ))\n",
        "        model.add(Dropout(rate=best_hyperparameters[f'fc_dropout_{i}']))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Learning rate and optimizer with weight decay\n",
        "    lr = best_hyperparameters['learning_rate']\n",
        "    wd = best_hyperparameters['weight_decay']\n",
        "    optimizer = AdamW(learning_rate=lr, weight_decay=wd)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and compile the final model with the best hyperparameters\n",
        "model = build_final_model()\n",
        "\n",
        "# Model summary to verify the architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3-rQort9rram",
        "outputId": "219e3361-5e68-4c43-d138-9cdd176af7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m36,992\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m147,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m590,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)                 │          \u001b[38;5;34m98,688\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m448\u001b[0m)                 │         \u001b[38;5;34m172,480\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m448\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m229,888\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m513\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,992</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">98,688</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">172,480</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">229,888</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,575,489\u001b[0m (6.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,489</span> (6.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,573,889\u001b[0m (6.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,889</span> (6.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,600\u001b[0m (6.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> (6.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TUODfCcUymD",
        "outputId": "0862558a-8c0e-40e0-d6c2-dd0685cd01fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing fold 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 82ms/step - accuracy: 0.7593 - loss: 0.7071\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8258 - loss: 0.5780\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8551 - loss: 0.5221\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8584 - loss: 0.4869\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8774 - loss: 0.4601\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8898 - loss: 0.4472\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8957 - loss: 0.4200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8893 - loss: 0.4433\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9068 - loss: 0.4027\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9067 - loss: 0.3944\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9119 - loss: 0.3890\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9112 - loss: 0.3765\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9041 - loss: 0.3841\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9241 - loss: 0.3608\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9236 - loss: 0.3471\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9259 - loss: 0.3508\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9339 - loss: 0.3263\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9395 - loss: 0.3167\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9418 - loss: 0.3144\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9393 - loss: 0.3201\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9428 - loss: 0.3084\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9486 - loss: 0.2970\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9510 - loss: 0.2862\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9593 - loss: 0.2611\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9575 - loss: 0.2799\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9581 - loss: 0.2703\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9559 - loss: 0.2762\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9648 - loss: 0.2573\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9680 - loss: 0.2589\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9572 - loss: 0.2708\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9713 - loss: 0.2392\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9609 - loss: 0.2601\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9660 - loss: 0.2453\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9686 - loss: 0.2284\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9662 - loss: 0.2376\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9678 - loss: 0.2366\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9779 - loss: 0.2188\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9776 - loss: 0.2107\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9726 - loss: 0.2229\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9682 - loss: 0.2335\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9782 - loss: 0.2090\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9780 - loss: 0.1985\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9802 - loss: 0.2065\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9818 - loss: 0.1940\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9829 - loss: 0.1935\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9806 - loss: 0.1966\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9793 - loss: 0.1972\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9823 - loss: 0.1905\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9818 - loss: 0.1866\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9825 - loss: 0.1928\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9862 - loss: 0.1757\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9829 - loss: 0.1806\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9874 - loss: 0.1701\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9868 - loss: 0.1722\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9899 - loss: 0.1639\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9850 - loss: 0.1720\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9836 - loss: 0.1733\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9850 - loss: 0.1730\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 80ms/step - accuracy: 0.9872 - loss: 0.1672\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9881 - loss: 0.1638\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9898 - loss: 0.1553\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9860 - loss: 0.1703\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9898 - loss: 0.1526\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9845 - loss: 0.1642\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9856 - loss: 0.1639\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9927 - loss: 0.1441\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9890 - loss: 0.1543\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9907 - loss: 0.1498\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9925 - loss: 0.1460\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9802 - loss: 0.1789\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9913 - loss: 0.1452\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9939 - loss: 0.1356\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9902 - loss: 0.1419\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9910 - loss: 0.1439\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9893 - loss: 0.1418\n",
            "Epoch 1/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 120ms/step - accuracy: 0.9927 - loss: 0.1349 - val_accuracy: 0.9641 - val_loss: 0.3353 - learning_rate: 6.6682e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9912 - loss: 0.1409 - val_accuracy: 0.9606 - val_loss: 0.4054 - learning_rate: 6.6682e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m185/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9923 - loss: 0.1358\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 6.668164860457183e-06.\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9923 - loss: 0.1358 - val_accuracy: 0.9500 - val_loss: 0.4641 - learning_rate: 6.6682e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9970 - loss: 0.1210 - val_accuracy: 0.9677 - val_loss: 0.3139 - learning_rate: 6.6682e-06\n",
            "Epoch 5/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9966 - loss: 0.1223 - val_accuracy: 0.9667 - val_loss: 0.3159 - learning_rate: 6.6682e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9976 - loss: 0.1180 - val_accuracy: 0.9727 - val_loss: 0.2739 - learning_rate: 6.6682e-06\n",
            "Epoch 7/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9981 - loss: 0.1175 - val_accuracy: 0.9707 - val_loss: 0.2861 - learning_rate: 6.6682e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m185/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9987 - loss: 0.1156\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 6.668165042356122e-07.\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.9987 - loss: 0.1155 - val_accuracy: 0.9742 - val_loss: 0.2745 - learning_rate: 6.6682e-06\n",
            "Epoch 9/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9985 - loss: 0.1157 - val_accuracy: 0.9737 - val_loss: 0.2773 - learning_rate: 6.6682e-07\n",
            "Epoch 9: early stopping\n",
            "Fold 1 - Validation Accuracy: 0.973737359046936 - Validation Loss: 0.2772792875766754\n",
            "New best model found for fold 1. Saved to /content/drive/MyDrive/Colab Notebooks/Model/BestModelwithCrossVal.keras\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step\n",
            "Processing fold 2/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 76ms/step - accuracy: 0.7071 - loss: 0.7757\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8416 - loss: 0.5597\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8518 - loss: 0.5118\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8657 - loss: 0.4986\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8838 - loss: 0.4639\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8940 - loss: 0.4437\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8943 - loss: 0.4312\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9036 - loss: 0.4092\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9009 - loss: 0.4159\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9084 - loss: 0.3890\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9169 - loss: 0.3836\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9197 - loss: 0.3660\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9221 - loss: 0.3639\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9181 - loss: 0.3569\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9297 - loss: 0.3392\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9351 - loss: 0.3252\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9343 - loss: 0.3261\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9313 - loss: 0.3343\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9420 - loss: 0.3112\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9380 - loss: 0.3149\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9462 - loss: 0.3017\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9454 - loss: 0.2945\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9470 - loss: 0.2945\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9489 - loss: 0.2837\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9545 - loss: 0.2843\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9524 - loss: 0.2759\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9563 - loss: 0.2691\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9644 - loss: 0.2559\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9632 - loss: 0.2467\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9612 - loss: 0.2597\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9611 - loss: 0.2578\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9650 - loss: 0.2507\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9686 - loss: 0.2336\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9730 - loss: 0.2290\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9693 - loss: 0.2324\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9688 - loss: 0.2268\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9703 - loss: 0.2281\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9797 - loss: 0.2105\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9708 - loss: 0.2240\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9799 - loss: 0.2046\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9746 - loss: 0.2169\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9743 - loss: 0.2191\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9746 - loss: 0.2045\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9827 - loss: 0.1908\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9750 - loss: 0.2222\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9836 - loss: 0.1866\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9822 - loss: 0.1897\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9795 - loss: 0.1948\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9838 - loss: 0.1887\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9835 - loss: 0.1836\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9856 - loss: 0.1830\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9853 - loss: 0.1759\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9849 - loss: 0.1768\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9861 - loss: 0.1782\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9865 - loss: 0.1746\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9829 - loss: 0.1759\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9892 - loss: 0.1675\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9872 - loss: 0.1738\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 75ms/step - accuracy: 0.9888 - loss: 0.1629\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9870 - loss: 0.1660\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9902 - loss: 0.1597\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9867 - loss: 0.1666\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9917 - loss: 0.1536\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9911 - loss: 0.1542\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9902 - loss: 0.1546\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9896 - loss: 0.1567\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9924 - loss: 0.1508\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9916 - loss: 0.1535\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9879 - loss: 0.1527\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9929 - loss: 0.1438\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9904 - loss: 0.1472\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9880 - loss: 0.1578\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9934 - loss: 0.1385\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9912 - loss: 0.1394\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9929 - loss: 0.1391\n",
            "Epoch 1/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 107ms/step - accuracy: 0.9931 - loss: 0.1369 - val_accuracy: 0.9662 - val_loss: 0.3361 - learning_rate: 6.6682e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9902 - loss: 0.1474 - val_accuracy: 0.9025 - val_loss: 0.8901 - learning_rate: 6.6682e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9878 - loss: 0.1461\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 6.668164860457183e-06.\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9878 - loss: 0.1460 - val_accuracy: 0.9444 - val_loss: 0.5015 - learning_rate: 6.6682e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9913 - loss: 0.1386 - val_accuracy: 0.9460 - val_loss: 0.4336 - learning_rate: 6.6682e-06\n",
            "Epoch 4: early stopping\n",
            "Fold 2 - Validation Accuracy: 0.9459595680236816 - Validation Loss: 0.43364736437797546\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step\n",
            "Processing fold 3/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 74ms/step - accuracy: 0.7364 - loss: 0.7350\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8357 - loss: 0.5641\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8629 - loss: 0.4916\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8769 - loss: 0.4642\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8850 - loss: 0.4368\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8983 - loss: 0.4251\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8995 - loss: 0.4200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9081 - loss: 0.4014\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9048 - loss: 0.4015\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9162 - loss: 0.3734\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9156 - loss: 0.3754\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9186 - loss: 0.3719\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9232 - loss: 0.3525\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9173 - loss: 0.3749\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9222 - loss: 0.3503\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9250 - loss: 0.3502\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9398 - loss: 0.3264\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9411 - loss: 0.3260\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9439 - loss: 0.3123\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9477 - loss: 0.3033\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9422 - loss: 0.3154\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9416 - loss: 0.2993\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9528 - loss: 0.2929\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9521 - loss: 0.2932\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9537 - loss: 0.2812\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9628 - loss: 0.2595\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9547 - loss: 0.2781\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9615 - loss: 0.2658\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9558 - loss: 0.2718\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9634 - loss: 0.2567\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9663 - loss: 0.2521\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9655 - loss: 0.2416\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9680 - loss: 0.2436\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9665 - loss: 0.2476\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9786 - loss: 0.2157\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9763 - loss: 0.2215\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9720 - loss: 0.2324\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9716 - loss: 0.2260\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9740 - loss: 0.2211\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9709 - loss: 0.2231\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9728 - loss: 0.2292\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9718 - loss: 0.2151\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9728 - loss: 0.2189\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9810 - loss: 0.1986\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9793 - loss: 0.2012\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9811 - loss: 0.2101\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9822 - loss: 0.1945\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9807 - loss: 0.1950\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9798 - loss: 0.1952\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9834 - loss: 0.1849\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9825 - loss: 0.1914\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9848 - loss: 0.1818\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9836 - loss: 0.1850\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9866 - loss: 0.1719\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9887 - loss: 0.1692\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9810 - loss: 0.1905\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9864 - loss: 0.1740\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9892 - loss: 0.1693\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9802 - loss: 0.1833\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9901 - loss: 0.1568\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9880 - loss: 0.1626\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9846 - loss: 0.1655\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9885 - loss: 0.1613\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9896 - loss: 0.1559\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9880 - loss: 0.1591\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9866 - loss: 0.1610\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9907 - loss: 0.1536\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9910 - loss: 0.1471\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9839 - loss: 0.1656\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9892 - loss: 0.1537\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9950 - loss: 0.1380\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9928 - loss: 0.1422\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9877 - loss: 0.1535\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9828 - loss: 0.1596\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9909 - loss: 0.1453\n",
            "Epoch 1/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 108ms/step - accuracy: 0.9912 - loss: 0.1436 - val_accuracy: 0.9429 - val_loss: 0.5546 - learning_rate: 6.6682e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9894 - loss: 0.1446 - val_accuracy: 0.9601 - val_loss: 0.3435 - learning_rate: 6.6682e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.9905 - loss: 0.1433 - val_accuracy: 0.9510 - val_loss: 0.4342 - learning_rate: 6.6682e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m185/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9926 - loss: 0.1365\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.668164860457183e-06.\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9926 - loss: 0.1364 - val_accuracy: 0.9470 - val_loss: 0.4314 - learning_rate: 6.6682e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9962 - loss: 0.1318 - val_accuracy: 0.9621 - val_loss: 0.3314 - learning_rate: 6.6682e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.9968 - loss: 0.1232 - val_accuracy: 0.9646 - val_loss: 0.3196 - learning_rate: 6.6682e-06\n",
            "Epoch 7/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9972 - loss: 0.1208 - val_accuracy: 0.9621 - val_loss: 0.3170 - learning_rate: 6.6682e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9982 - loss: 0.1188 - val_accuracy: 0.9621 - val_loss: 0.3340 - learning_rate: 6.6682e-06\n",
            "Epoch 9/10\n",
            "\u001b[1m185/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9976 - loss: 0.1208\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 6.668165042356122e-07.\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9976 - loss: 0.1208 - val_accuracy: 0.9545 - val_loss: 0.3934 - learning_rate: 6.6682e-06\n",
            "Epoch 10/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9975 - loss: 0.1203 - val_accuracy: 0.9566 - val_loss: 0.3730 - learning_rate: 6.6682e-07\n",
            "Epoch 10: early stopping\n",
            "Fold 3 - Validation Accuracy: 0.9565656781196594 - Validation Loss: 0.37295445799827576\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step\n",
            "Processing fold 4/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 118ms/step - accuracy: 0.7359 - loss: 0.7109\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8319 - loss: 0.5418\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8526 - loss: 0.5076\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8811 - loss: 0.4675\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8833 - loss: 0.4423\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8840 - loss: 0.4338\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9001 - loss: 0.4025\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9027 - loss: 0.3921\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9088 - loss: 0.3971\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9182 - loss: 0.3705\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9214 - loss: 0.3680\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9250 - loss: 0.3542\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9308 - loss: 0.3458\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9333 - loss: 0.3453\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9337 - loss: 0.3197\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9434 - loss: 0.3178\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9409 - loss: 0.3276\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9431 - loss: 0.3130\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9471 - loss: 0.3028\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9466 - loss: 0.3102\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9517 - loss: 0.2847\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9585 - loss: 0.2830\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9565 - loss: 0.2718\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9628 - loss: 0.2697\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9613 - loss: 0.2622\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9585 - loss: 0.2621\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9638 - loss: 0.2535\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9679 - loss: 0.2463\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - accuracy: 0.9672 - loss: 0.2481\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9718 - loss: 0.2373\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9720 - loss: 0.2343\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9704 - loss: 0.2355\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9741 - loss: 0.2310\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9784 - loss: 0.2171\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9719 - loss: 0.2250\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9761 - loss: 0.2142\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9826 - loss: 0.2021\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - accuracy: 0.9727 - loss: 0.2224\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9803 - loss: 0.2091\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9836 - loss: 0.1947\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9777 - loss: 0.2037\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9845 - loss: 0.2013\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9885 - loss: 0.1825\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9823 - loss: 0.1970\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9858 - loss: 0.1806\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9867 - loss: 0.1812\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - accuracy: 0.9855 - loss: 0.1820\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9800 - loss: 0.1954\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9888 - loss: 0.1701\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9825 - loss: 0.1862\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9883 - loss: 0.1715\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9817 - loss: 0.1865\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9865 - loss: 0.1777\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9881 - loss: 0.1682\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9909 - loss: 0.1679\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9899 - loss: 0.1730\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9875 - loss: 0.1689\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9879 - loss: 0.1699\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 124ms/step - accuracy: 0.9889 - loss: 0.1583\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9897 - loss: 0.1616\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9918 - loss: 0.1552\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9926 - loss: 0.1500\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9886 - loss: 0.1558\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9886 - loss: 0.1549\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9911 - loss: 0.1506\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9893 - loss: 0.1537\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9909 - loss: 0.1502\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9937 - loss: 0.1414\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9869 - loss: 0.1542\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9949 - loss: 0.1357\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9904 - loss: 0.1533\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9945 - loss: 0.1329\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9872 - loss: 0.1640\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9955 - loss: 0.1306\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9929 - loss: 0.1380\n",
            "Epoch 1/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 142ms/step - accuracy: 0.9922 - loss: 0.1362 - val_accuracy: 0.9586 - val_loss: 0.3795 - learning_rate: 6.6682e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.9947 - loss: 0.1250 - val_accuracy: 0.9651 - val_loss: 0.3420 - learning_rate: 6.6682e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9906 - loss: 0.1394 - val_accuracy: 0.9545 - val_loss: 0.4175 - learning_rate: 6.6682e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m185/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9884 - loss: 0.1458\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.668164860457183e-06.\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.9884 - loss: 0.1458 - val_accuracy: 0.9464 - val_loss: 0.4485 - learning_rate: 6.6682e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.9941 - loss: 0.1310 - val_accuracy: 0.9687 - val_loss: 0.3047 - learning_rate: 6.6682e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.9969 - loss: 0.1210 - val_accuracy: 0.9687 - val_loss: 0.3104 - learning_rate: 6.6682e-06\n",
            "Epoch 7/10\n",
            "\u001b[1m185/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9989 - loss: 0.1169\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.668165042356122e-07.\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.9989 - loss: 0.1169 - val_accuracy: 0.9687 - val_loss: 0.3195 - learning_rate: 6.6682e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.9973 - loss: 0.1191 - val_accuracy: 0.9682 - val_loss: 0.3147 - learning_rate: 6.6682e-07\n",
            "Epoch 8: early stopping\n",
            "Fold 4 - Validation Accuracy: 0.9681657552719116 - Validation Loss: 0.31474563479423523\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step\n",
            "Best model saved to /content/drive/MyDrive/Colab Notebooks/Model/BestModelwithCrossVal.keras\n"
          ]
        }
      ],
      "source": [
        "# Number of folds for cross-validation\n",
        "n_splits = 4\n",
        "best_val_accuracy=0.0\n",
        "# Prepare stratified k-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "best_model_path = '/content/drive/MyDrive/Colab Notebooks/Model/BestModelwithCrossVal.keras'\n",
        "# Data augmentation configuration for the minority class\n",
        "mindatagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.02,\n",
        "    height_shift_range=0.02,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Initialize lists to collect validation scores and metrics\n",
        "validation_accuracies = []\n",
        "confusion_matrices = []\n",
        "classification_reports = []\n",
        "roc_auc_scores = []\n",
        "precision_recall_values = []\n",
        "histories = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(standardized_images_np, lbl_encoded)):\n",
        "    print(f\"Processing fold {fold + 1}/{n_splits}\")\n",
        "\n",
        "    # Split the data into training and validation sets for this fold\n",
        "    X_train_fold, X_val_fold = standardized_images_np[train_index], standardized_images_np[val_index]\n",
        "    y_train_fold, y_val_fold = lbl_encoded[train_index], lbl_encoded[val_index]\n",
        "\n",
        "    # Separate the minority and majority class in the training set of this fold\n",
        "    minority_images = X_train_fold[y_train_fold == 0]\n",
        "    minority_labels = y_train_fold[y_train_fold == 0]\n",
        "    majority_count = np.sum(y_train_fold == 1)\n",
        "\n",
        "    # Calculate the number of additional samples needed for the minority class\n",
        "    additional_samples_needed = int(0.05 * majority_count) + (majority_count - len(minority_labels))\n",
        "\n",
        "    # Create a data generator for the minority class\n",
        "    minority_generator = mindatagen.flow(\n",
        "        minority_images,\n",
        "        minority_labels,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    # Train the model on the augmented training set\n",
        "    model = build_final_model()\n",
        "\n",
        "    # Determine number of batches needed\n",
        "    n_batches = additional_samples_needed // 32\n",
        "\n",
        "    for batch in range(n_batches):\n",
        "        imgs, lbls = next(minority_generator)\n",
        "        X_train_batch = np.concatenate([X_train_fold, imgs])\n",
        "        y_train_batch = np.concatenate([y_train_fold, lbls])\n",
        "\n",
        "        # Train the model on the batch\n",
        "        model.fit(\n",
        "            X_train_batch, y_train_batch,\n",
        "            epochs=1,  # Train for one epoch at a time\n",
        "            batch_size=32,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    # After augmenting, train on the original data again\n",
        "    history=model.fit(\n",
        "        X_train_fold, y_train_fold,\n",
        "        epochs=10,  # Adjust epochs as needed\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        class_weight=class_weights_dict,\n",
        "        callbacks=[\n",
        "            EarlyStopping(patience=3, verbose=1),\n",
        "            ReduceLROnPlateau(patience=2, verbose=1)\n",
        "        ],\n",
        "        verbose=1\n",
        "    )\n",
        "    # Store the history object\n",
        "    histories.append(history)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "    validation_accuracies.append(val_accuracy)\n",
        "    print(f\"Fold {fold + 1} - Validation Accuracy: {val_accuracy} - Validation Loss: {val_loss}\")\n",
        "\n",
        "    # Check if this model is the best so far\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model = model\n",
        "        best_model.save(best_model_path)\n",
        "        print(f\"New best model found for fold {fold + 1}. Saved to {best_model_path}\")\n",
        "\n",
        "    # Predictions on the validation set\n",
        "    y_val_pred = model.predict(X_val_fold).round().astype(int)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_val_fold, y_val_pred)\n",
        "    confusion_matrices.append(cm)\n",
        "\n",
        "    # Classification Report\n",
        "    report = classification_report(y_val_fold, y_val_pred, output_dict=True)\n",
        "    classification_reports.append(report)\n",
        "\n",
        "    # ROC-AUC Score\n",
        "    roc_auc = roc_auc_score(y_val_fold, y_val_pred)\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_val_fold, y_val_pred)\n",
        "    precision_recall_values.append((precision, recall))\n",
        "\n",
        "# Save the best model\n",
        "print(f\"Best model saved to {best_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Calculate the average confusion matrix\n",
        "average_cm = np.mean(confusion_matrices, axis=0).astype(int)  # Ensure the average confusion matrix has integer values\n",
        "#Display the average confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm, display_labels=['Class 0', 'Class 1'])  # Adjust labels as necessary\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Average Confusion Matrix for DCNN classifier')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the average validation accuracy across all folds\n",
        "average_validation_score = np.mean(validation_accuracies)\n",
        "print(f\"Average Validation Accuracy: {average_validation_score}\")\n",
        "\n",
        "# Calculate the average classification report metrics\n",
        "average_precision = np.mean([report['1']['precision'] for report in classification_reports])\n",
        "average_recall = np.mean([report['1']['recall'] for report in classification_reports])\n",
        "average_f1_score = np.mean([report['1']['f1-score'] for report in classification_reports])\n",
        "print(f\"Average Precision: {average_precision}\")\n",
        "print(f\"Average Recall: {average_recall}\")\n",
        "print(f\"Average F1-Score: {average_f1_score}\")\n",
        "\n",
        "# Average ROC-AUC Score\n",
        "average_roc_auc = np.mean(roc_auc_scores)\n",
        "print(f\"Average ROC-AUC Score: {average_roc_auc}\")\n",
        "\n",
        "# Average Precision-Recall Curve\n",
        "average_precision = np.mean([pr[0] for pr in precision_recall_values], axis=0)\n",
        "average_recall = np.mean([pr[1] for pr in precision_recall_values], axis=0)\n",
        "print(f\"Average Precision-Recall Values:\\nPrecision: {average_precision}\\nRecall: {average_recall}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "HdUYoUgXU4ed",
        "outputId": "41b80062-4def-40fe-9186-3691c2acb8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Accuracy: 0.9611070901155472\n",
            "Average Precision: 0.9879767941820166\n",
            "Average Recall: 0.9551412424793317\n",
            "Average F1-Score: 0.9711196928249964\n",
            "Average ROC-AUC Score: 0.9646673954332142\n",
            "Average Precision-Recall Values:\n",
            "Precision: [0.68682913 0.98797679 1.        ]\n",
            "Recall: [1.         0.95514124 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jpYG5WYfSD4m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}